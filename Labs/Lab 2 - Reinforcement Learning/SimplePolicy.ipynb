{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIC_rg6InAZI"
      },
      "source": [
        "## Multiarmed bandits with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDy7CKYaah-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44004fd-eff5-420f-f03b-ae343cdb101f"
      },
      "source": [
        "!pip install tensorflow==1.14\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.14\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 44 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.5.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.47.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.8.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj8yxBKgnAZJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_DOCBWnAZO"
      },
      "source": [
        "### Bandits\n",
        "The pullBandit function generates a random number from a normal distribution with an average value of 0. The fewer the number of bandits, the more likely it is to receive a positive reward.\n",
        "\n",
        "Objective: to learn always to choose a bandit who will give a positive reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc9UwGV2nAZP"
      },
      "source": [
        "# A list of bandits with initialization of the values representing probabilities of winning \n",
        "bandits = [0.8, 0.5, -0.1, -0.8, -2.5]\n",
        "num_bandits = len(bandits)\n",
        "def pullBandit(bandit):\n",
        "    # random number\n",
        "    result = np.random.randn(1)\n",
        "    if result > bandit:\n",
        "        # reward\n",
        "        return 1\n",
        "    else:\n",
        "        # losing\n",
        "        return -1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfDnNwpanAZS"
      },
      "source": [
        "### Arm-pulling agent\n",
        "\n",
        "Initialization of the agent and its behavior.\n",
        "The environment consists of a set of values for each of the bandits. Each value is an estimate of the profit earned from choosing a bandit.\n",
        "\n",
        "The gradient descent method is used to update the agent's state based on the reward received."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0RH96ElnAZS"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# choosing the arm\n",
        "weights = tf.Variable(tf.ones([num_bandits])) # at the beginning all the arms have equal weights\n",
        "chosen_action = tf.argmax(weights,0) # choose the arm with the max weight at the moment\n",
        "\n",
        "# placeholders\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "responsible_weight = tf.slice(weights,action_holder,[1]) # take the corresponding weight of the arm according to the action\n",
        "loss = -(tf.log(responsible_weight)*reward_holder) # as the reward rises the loss decreses\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYps3DGWnAZW"
      },
      "source": [
        "### Agent training\n",
        "\n",
        "We will train the agent by acting in an initialized environment and receiving a reward. While learning, the agent is more likely to choose actions that will bring greater reward over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUmHTz_3nAZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3029ce0c-58b7-4ffc-d440-0a9c181c69df"
      },
      "source": [
        "total_episodes = 1000 # Number of episodes of game\n",
        "total_reward = np.zeros(num_bandits) # Score = 0\n",
        "e = 0.05 # probability of taking random steps\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Start computing the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while i < total_episodes:\n",
        "        \n",
        "        # Exploration vs. Exploitation. With probability e we take a random action.\n",
        "        # Choose the arm\n",
        "        if np.random.rand(1) < e:\n",
        "            action = np.random.randint(num_bandits)\n",
        "        else:\n",
        "            action = sess.run(chosen_action)\n",
        "        \n",
        "        reward = pullBandit(bandits[action]) # Rewarding\n",
        "        \n",
        "        # Update the weights\n",
        "        _, resp, ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
        "        \n",
        "        # Update the reward sum\n",
        "        total_reward[action] += reward\n",
        "        if i % 50 == 0:\n",
        "            print (\"Current reward \" + str(total_reward))\n",
        "        i+=1\n",
        "print (\"\\nAgent say that the arm № \" + str(np.argmax(ww)+1) + \" is the most appropriate!\")\n",
        "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
        "    print (\"...and it was right!\")\n",
        "else:\n",
        "    print (\"...and it was wrong!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current reward [-1.  0.  0.  0.  0.]\n",
            "Current reward [-1. -2.  1.  0. 13.]\n",
            "Current reward [ 0. -2.  1.  0. 62.]\n",
            "Current reward [  0.  -2.   0.   1. 106.]\n",
            "Current reward [  0.  -2.  -1.   1. 155.]\n",
            "Current reward [  0.  -2.  -2.   1. 204.]\n",
            "Current reward [  0.  -3.  -2.   1. 253.]\n",
            "Current reward [  0.  -3.  -2.   2. 302.]\n",
            "Current reward [ -2.  -4.  -2.   3. 348.]\n",
            "Current reward [ -2.  -3.  -1.   3. 396.]\n",
            "Current reward [ -2.  -4.  -1.   3. 445.]\n",
            "Current reward [ -2.  -4.   1.   3. 493.]\n",
            "Current reward [ -3.  -4.   2.   3. 539.]\n",
            "Current reward [ -3.  -4.   2.   4. 586.]\n",
            "Current reward [ -3.  -5.   2.   4. 635.]\n",
            "Current reward [ -3.  -5.   2.   4. 685.]\n",
            "Current reward [ -2.  -5.   2.   4. 734.]\n",
            "Current reward [ -3.  -5.   3.   4. 780.]\n",
            "Current reward [ -3.  -6.   3.   4. 827.]\n",
            "Current reward [ -4.  -7.   4.   4. 872.]\n",
            "\n",
            "Agent say that the arm № 5 is the most appropriate!\n",
            "...and it was right!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8u45vmSbRmv"
      },
      "source": [],
      "execution_count": 10,
      "outputs": []
    }
  ]
}